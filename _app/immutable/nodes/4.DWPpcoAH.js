import{e as M,a as s,t as b,b as o,s as w}from"../chunks/oucrhdpv.js";import"../chunks/BeWUZLz4.js";import{c as l,r as n,t as A,f as le,s as a,n as i,g as f,$ as $e}from"../chunks/D8_uhm_e.js";import{l as de,p as we,i as Ae}from"../chunks/Cx6OMC2R.js";import{e as G,i as Q}from"../chunks/B4bTylva.js";import{b as p}from"../chunks/DJhHUK4p.js";import{B as ze,a as Ne}from"../chunks/DX94DZ0p.js";import{O as y}from"../chunks/Cy5wPpce.js";import{b as ke,c as R,a as Pe,t as ce,L as c}from"../chunks/CKa-MYti.js";import{T as me}from"../chunks/DNcivkqL.js";var Fe=b("<div><!></div>");function Me(L,u){const T=de(u,["children","$$slots","$$events","$$legacy"]),z=de(T,["light"]);let N=we(u,"light",8,!1);var d=Fe();let v;var S=l(d);ke(S,u,"default",{},null),n(d),A(()=>{v=Pe(d,v,{...z}),ce(d,"bx--tile",!0),ce(d,"bx--tile--light",N())}),M("click",d,function(m){R.call(this,u,m)}),M("mouseover",d,function(m){R.call(this,u,m)}),M("mouseenter",d,function(m){R.call(this,u,m)}),M("mouseleave",d,function(m){R.call(this,u,m)}),s(L,d)}var Re=b("<!>&nbsp;",1),Le=b('<span class="text-xs block py-2 font-mono">M/S models used: <!></span>'),Te=b('<dl class="w-full"><dt class="font-semibold text-base"> </dt> <dd class="py-2 leading-normal"> <!></dd></dl>'),Se=b('<div class="flex gap-6 py-4"><div class="min-w-56"><!></div> <div><p class="max-w-[85ch] text-sm"> </p></div></div>'),qe=b('<!> <div class="max-w-[85ch]"><h1 class="mb-8 md:text-5xl">M/S Data Guide</h1> <div class="my-6"><h2 class="mb-2">Contents</h2> <ul class="list-inside list-disc [&amp;_ul]:list-[revert]"><li class="pt-2"><a href="#introduction">Introduction</a> <ul class="list-inside indent-4"><li class="pt-2"><a href="#training-set">Training Set</a></li> <li class="pt-2"><a href="#metadata-sources">Metadata Sources</a></li> <li class="pt-2"><a href="#metadata-note">Metadata on Sex and Complexion</a></li></ul></li> <li class="pt-2"><a href="#methods-glossary">Methods Glossary</a></li> <li class="pt-2"><a href="#tags">Data Provenance Tags</a></li></ul></div> <h2 id="introduction" class="my-4">Introduction</h2> <h3 id="training-set" class="mb-4 mt-6 text-base uppercase tracking-widest md:text-lg">Training Set</h3> <p class="py-2">This M/S Reading Room prototype allows users to search the test set of A-Files using metadata derived from two sources: the publicly available U.S. National Archives (NARA) catalog metadata, and metadata based on information extracted by our machine learning models.</p> <p class="py-2">Most of the training set is made up of files ordered from the Kansas City regional office of NARA, where the vast majority of public domain A-Files are held. While these A-files are in the public domain (A-files enter the public domain when the birth year of the file holder is at least 100 years in the past), NARA reviews each file for third-party privacy violations and redacts information as necessary. NARA levies a processing  fee of either $27 or $40 (depending on the file holder’s year of birth) for each A-File ordered online. Faced with a catalog of a million files and a budget for 550 files, we made selections designed to achieve regional and temporal breadth, with additional files for two countries of origin (Mexico and Haiti) to enable greater depth of research. The remainder of the training set comes from <!> and a relatively small number of A-Files obtained from USCIS via that agency’s Electronic Reading Room and our Freedom of Information Act (FOIA) requests for the A-Files of verifiably deceased individuals. The USCIS files are being used for model training purposes only and are not included in this prototype.</p> <h3 id="metadata-sources" class="mb-4 mt-6 text-base uppercase tracking-widest md:text-lg">Metadata Sources</h3> <p class="py-2">The <!> for A-Files varies.  Every five years, files that have entered the public domain are transferred from the federal agency governing immigration (INS or USCIS) to NARA. At the time of  the transfer, the agencies determine which metadata fields attached to the files will be transferred, resulting in variability in metadata across the entire NARA catalog (approximately 1.6 million A-Files). The basic metadata always available from the NARA catalog for each A-File includes date of birth, first name, last name, and country of birth. Additional metadata from the NARA catalog can include sex, date of entry, port of entry, date file opened, father’s name, mother’s name, date and location of naturalization, naturalization certificate number, and class of admission. Reference tables are available to identify the agency codes used for <!>, <!>, and <!>.</p> <p class="py-2">The metadata created by the <span class="italic">Migrants and the State</span> project has been derived using a variety of machine learning methods deployed on a training set of 751 files comprising 37,396 individual pages. The model training  involved two main stages: categorizing documents and extracting information from them. In general, our approach has prioritized training models to address a larger number of data identification tasks rather than attempting fewer tasks and refining the models to achieve peak accuracy. We started by developing a model (using <!>) to create image embeddings for every page in the training set. Members of the team reviewed each page to identify machine-generated errors and used that information to improve the model’s performance in distinguishing among <!>, <!>, <!>, and other kinds of materials (which we labelled <!>).</p> <p class="py-2">While the model identifying document types relied on the visual information on each A-File page, the ability to extract text information required the use of <!> to create a machine-readable text version of the training set. This text allows us to perform general text extraction (identifying the presence of text in the body of a document, similar to a keyword search) as well as extracting information from a specific location within a document (for example, a form title or answer to a question).</p> <p class="py-2">We used existing <!> techniques to extract information about the years and country names included in each A-File. For document-specific metadata extraction, we chose the G-325 and Certificate of Naturalization forms as our test cases for training models (using a Large Language Model approach) that can identify a form type and extract specific information from it. The information the models can extract from these forms (employment history, residential addresses, and biographical details) is valuable in its own right, but we are also attempting to develop a scalable technique that can later be applied to other forms.</p> <h3 id="metadata-note" class="mb-4 mt-6 text-base uppercase tracking-widest md:text-lg">Metadata on Sex and Complexion</h3> <p class="py-2">The NARA catalog metadata, compiled as a limited subset of the metadata created by INS / USCIS, often lacks entries corresponding to the field designating biological sex. But information on biological sex, along with other biometric data required by government forms, appears frequently across documents in A-Files. We used an open-source <!> to extract this data from the individual pages of A-Files, typically from forms where a biological sex designation (e.g.  Male / Female) is required. The model then aggregates the total number of identifications for biological sex and predicts the sex of the A-File holder. The LLM derived metadata is not 100% accurate, and the prototype allows you to compare NARA’s metadata for sex with the metadata extracted by the M/S trained LLM.</p> <p class="py-2">Extracting metadata to reflect historical government categories and bureaucratic norms around biological sex is relatively straightforward, given the binary categories of male and female then in use. Extracting metadata about ethno-racial categorization is more complex. This information often takes the form of indications of a file holder’s complexion, for which migrants and immigration officers used a variety of adjectives, many of them offensive to contemporary sensibilities. Currently, our models extract complexion data only from the Certificate of Naturalization forms. From the subset of data available in this prototype, complexion was recorded directly in these documents as <!>, <!>, or <!>, if at all. Our models attempt to record these terms exactly or add an <!> value if they do not find a complexion value on the page.</p> <p class="py-2">We crucially do not try to infer or profile a value through other cues in the A-File. Our aim is to organize the specific terms used by immigration bureaucracies over time as objects of study without intervention or curation. Nevertheless, please note that these metadata values, like others generated by machine learning in our corpus, are not 100% accurate. Additionally, more terms will likely emerge as we incorporate A-Files from broader time periods. For these reasons, we plan to solicit and incorporate user feedback to improve how we render this data with care.</p></div> <h2 id="methods-glossary" class="py-4">Methods Glossary</h2> <div class="my-10 grid sm:grid-cols-2 md:grid-cols-3 gap-4 w-full"></div> <h2 id="tags" class="py-4">Data Provenance Tags</h2> <div class="w-full"><div class="flex gap-6 py-4"><div class="min-w-56"><!></div> <div><p class="max-w-[85ch] text-sm">Fields with this tag come directly from the U.S. National Archives catalog.</p></div></div> <!></div>',1);function He(L){const u=[{id:"dnn",term:"Deep Neural Network (DNN)",description:"Analyzes and extracts important details from an image, such as shapes, colors, and patterns. Based on the analysis, the DNN then predicts how to classify the image.",models:[{name:"DINOv2",link:"https://github.com/facebookresearch/dinov2"}]},{id:"llm",term:"Large Language Model (LLM)",description:"A type of artificial intelligence trained on vast amounts of images and text to understand and generate human-like language while also interpreting visual information.  They can analyze and describe images, answer questions about visual content, and generate text based on both visual and textual inputs. This allows them to perform tasks like image captioning, visual question answering, and multimodal reasoning.",models:[{name:"MiniCPM-V-2_6",link:"https://huggingface.co/openbmb/MiniCPM-V-2_6"},{name:"Qwen2-VL-2B-Instruct",link:"https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct"}]},{id:"ml",term:"Machine Learning (ML)",description:"A type of artificial intelligence where computers learn patterns from data instead of being explicitly programmed."},{id:"ner",term:"Named Entity Recognition (NER)",description:"An NLP technique that identifies different types of entities such as locations, names, dates, and times by analyzing the surrounding context and linguistic patterns rather than relying solely on predefined rules like capitalization or keyword proximity.",models:[{name:"English NER in Flair",link:"https://huggingface.co/flair/ner-english-ontonotes-large"}]},{id:"nlp",term:"Natural Language Processing (NLP)",description:"A set of artificial intelligence techniques that enable computers to identify, understand, interpret, and generate human language."},{id:"ocr",term:"Optical Character Recognition (OCR)",description:"A technology that detects and extracts text from images or scanned documents, converting it into machine-readable text.",models:[{name:"Amazon Textract",link:"https://aws.amazon.com/textract/"}]},{id:"text",term:"Text Model",description:"An artificial intelligence system designed to process, generate, or analyze human language using machine learning techniques. In this case,  a sentence transformer that converts sentences into numerical embeddings, enabling tasks like semantic search, similarity comparison, and clustering.",models:[{name:"all-mpnet-base-v2",link:"https://huggingface.co/sentence-transformers/all-mpnet-base-v2"}]}],T={};var z=qe(),N=le(z);ze(N,{class:"mb-8",children:(t,r)=>{Ne(t,{href:p,children:(e,k)=>{i();var h=o("Home");s(e,h)},$$slots:{default:!0}})},$$slots:{default:!0}});var d=a(N,2),v=a(l(d),10),S=a(l(v));y(S,{size:"lg",href:"https://catalog.archives.gov/search?typeOfMaterials=Textual%20Records&availableOnline=true&recordGroupNumber=566",children:(t,r)=>{i();var e=o("A-Files made public by NARA’s San Francisco regional office");s(t,e)},$$slots:{default:!0}}),i(),n(v);var m=a(v,4),H=a(l(m));y(H,{size:"lg",href:"https://docs.google.com/document/u/0/d/1lnvN77_CaBnOzg9HBF-jtWTwJlisgKoD3E1RKHwQByQ/",children:(t,r)=>{i();var e=o("metadata available in the NARA catalog");s(t,e)},$$slots:{default:!0}});var V=a(H,2);y(V,{size:"lg",href:"https://docs.google.com/spreadsheets/d/1LfmR0QLmalz_6rJT_37QjRyFggsCXXOl5e24nOuw6XI/",children:(t,r)=>{i();var e=o("countries");s(t,e)},$$slots:{default:!0}});var X=a(V,2);y(X,{size:"lg",href:"https://docs.google.com/spreadsheets/d/1Zhs2RUElDE96EJmXGOCogfUdukj6mpbhWwsd6e2Sb34/",children:(t,r)=>{i();var e=o("ports of entry");s(t,e)},$$slots:{default:!0}});var he=a(X,2);y(he,{size:"lg",href:"https://docs.google.com/spreadsheets/d/1DH_0Wf0ALAB72D-v1Gu3sl7Iv_d6doSMv5-jj_r5Crc/",children:(t,r)=>{i();var e=o("naturalization locations");s(t,e)},$$slots:{default:!0}}),i(),n(m);var q=a(m,2),J=a(l(q),3);c(J,{size:"lg",href:"#dnn",children:(t,r)=>{i();var e=o("Deep Neural Networks");s(t,e)},$$slots:{default:!0}});var K=a(J,2);c(K,{size:"lg",href:`${p??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=form`,children:(t,r)=>{i();var e=o("forms");s(t,e)},$$slots:{default:!0}});var Z=a(K,2);c(Z,{size:"lg",href:`${p??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=letter`,children:(t,r)=>{i();var e=o("letters");s(t,e)},$$slots:{default:!0}});var Y=a(Z,2);c(Y,{size:"lg",href:`${p??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=photograph`,children:(t,r)=>{i();var e=o("photographs");s(t,e)},$$slots:{default:!0}});var fe=a(Y,2);c(fe,{size:"lg",href:`${p??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=misc`,children:(t,r)=>{i();var e=o("“misc”");s(t,e)},$$slots:{default:!0}}),i(),n(q);var C=a(q,2),ue=a(l(C));c(ue,{size:"lg",href:"#ocr",children:(t,r)=>{i();var e=o("Optical Character Recognition (OCR)");s(t,e)},$$slots:{default:!0}}),i(),n(C);var I=a(C,2),pe=a(l(I));c(pe,{size:"lg",href:"#ner",children:(t,r)=>{i();var e=o("Named Entity Recognition (NER)");s(t,e)},$$slots:{default:!0}}),i(),n(I);var O=a(I,4),ge=a(l(O));c(ge,{size:"lg",href:"#llm",children:(t,r)=>{i();var e=o("Large Language Model (LLM)");s(t,e)},$$slots:{default:!0}}),i(),n(O);var ee=a(O,2),te=a(l(ee));c(te,{size:"lg",href:`${p??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=medium`,children:(t,r)=>{i();var e=o("medium");s(t,e)},$$slots:{default:!0}});var ae=a(te,2);c(ae,{size:"lg",href:`${p??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=dark`,children:(t,r)=>{i();var e=o("dark");s(t,e)},$$slots:{default:!0}});var ie=a(ae,2);c(ie,{size:"lg",href:`${p??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=fair`,children:(t,r)=>{i();var e=o("fair");s(t,e)},$$slots:{default:!0}});var ve=a(ie,2);c(ve,{size:"lg",href:`${p??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=n.a`,children:(t,r)=>{i();var e=o("N.A");s(t,e)},$$slots:{default:!0}}),i(),n(ee),i(2),n(d);var E=a(d,4);G(E,5,()=>u,Q,(t,r)=>{Me(t,{get id(){return f(r).id},children:(e,k)=>{var h=Te(),g=l(h),B=l(g,!0);n(g);var P=a(g,2),_=l(P),F=a(_);Ae(F,()=>f(r).models,j=>{var x=Le(),oe=a(l(x));G(oe,1,()=>f(r).models,Q,(W,U)=>{var $=Re(),_e=le($);y(_e,{size:"sm",get href(){return f(U).link},children:(xe,Ce)=>{i();var ne=o();A(()=>w(ne,f(U).name)),s(xe,ne)},$$slots:{default:!0}}),i(),s(W,$)}),n(x),s(j,x)}),n(P),n(h),A(()=>{w(B,f(r).term),w(_,`${f(r).description??""} `)}),s(e,h)},$$slots:{default:!0}})}),n(E);var re=a(E,4),D=l(re),se=l(D),ye=l(se);c(ye,{href:"#nara",children:(t,r)=>{me(t,{interactive:!0,id:"nara",type:"blue",children:(e,k)=>{i();var h=o("#nara");s(e,h)},$$slots:{default:!0}})},$$slots:{default:!0}}),n(se),i(2),n(D);var be=a(D,2);G(be,1,()=>Object.entries(T),Q,(t,r)=>{let e=()=>f(r)[0],k=()=>f(r)[1];var h=Se(),g=l(h),B=l(g),P=$e(()=>`#${e()}`);c(B,{get href(){return f(P)},children:(x,oe)=>{me(x,{interactive:!0,get id(){return e()},type:"green",children:(W,U)=>{i();var $=o();A(()=>w($,`#${e()??""}`)),s(W,$)},$$slots:{default:!0}})},$$slots:{default:!0}}),n(g);var _=a(g,2),F=l(_),j=l(F,!0);n(F),n(_),n(h),A(()=>w(j,k())),s(t,h)}),n(re),s(L,z)}export{He as component};
