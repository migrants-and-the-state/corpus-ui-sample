import{e as E,a as r,t as k,n as _e,s as M,c as Se,b as d}from"../chunks/78UBv2sO.js";import{i as Ce}from"../chunks/QL-qlAGj.js";import{c as l,r as o,t as A,p as Te,l as ue,a as qe,b as Ie,n as i,d as pe,e as K,g as f,m as ge,f as ee,s as a,ah as Oe}from"../chunks/BB2ehQH7.js";import{l as N,p as te,i as be,s as Ee}from"../chunks/C84HHPft.js";import{e as Z,i as Y}from"../chunks/B3quC60w.js";import{b as _}from"../chunks/BQ26m2y_.js";import{c as xe,b as x,a as $e,t as ve,L as ae}from"../chunks/hz7f5l6I.js";import{O as L}from"../chunks/85ISIHUc.js";import{S as b}from"../chunks/B81sKpYZ.js";import{B as Be,a as De}from"../chunks/DOPufB3f.js";import{T as ye}from"../chunks/CGTpk8vp.js";var Ge=k("<div><!></div>");function je($,n){const u=N(n,["children","$$slots","$$events","$$legacy"]),y=N(u,["light"]);let c=te(n,"light",8,!1);var m=Ge();let p;var g=l(m);xe(g,n,"default",{},null),o(m),A(()=>{p=$e(m,p,{...y}),ve(m,"bx--tile",!0),ve(m,"bx--tile--light",c())}),E("click",m,function(h){x.call(this,n,h)}),E("mouseover",m,function(h){x.call(this,n,h)}),E("mouseenter",m,function(h){x.call(this,n,h)}),E("mouseleave",m,function(h){x.call(this,n,h)}),r($,m)}var We=_e("<title> </title>"),He=_e('<svg><!><path d="M17 22L17 14 13 14 13 16 15 16 15 22 12 22 12 24 20 24 20 22 17 22zM16 8a1.5 1.5 0 101.5 1.5A1.5 1.5 0 0016 8z"></path><path d="M26,28H6a2.0023,2.0023,0,0,1-2-2V6A2.0023,2.0023,0,0,1,6,4H26a2.0023,2.0023,0,0,1,2,2V26A2.0023,2.0023,0,0,1,26,28ZM6,6V26H26V6Z"></path></svg>');function we($,n){const u=N(n,["children","$$slots","$$events","$$legacy"]),y=N(u,["size","title"]);Te(n,!1);const c=ge(),m=ge();let p=te(n,"size",8,16),g=te(n,"title",8,void 0);ue(()=>(K(u),K(g())),()=>{pe(c,u["aria-label"]||u["aria-labelledby"]||g())}),ue(()=>(f(c),K(u)),()=>{pe(m,{"aria-hidden":f(c)?void 0:!0,role:f(c)?"img":void 0,focusable:Number(u.tabindex)===0?!0:void 0})}),qe(),Ce();var h=He();let F;var D=l(h);be(D,g,P=>{var z=We(),T=l(z,!0);o(z),A(()=>M(T,g())),r(P,z)}),i(2),o(h),A(()=>F=$e(h,F,{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 32 32",fill:"currentColor",preserveAspectRatio:"xMidYMid meet",width:p(),height:p(),...f(m),...y},void 0,!0)),r($,h),Ie()}function B($,n){const u=N(n,["children","$$slots","$$events","$$legacy"]),y=N(u,[]);ae($,Ee(()=>y,{icon:we,$$events:{click(c){x.call(this,n,c)},mouseover(c){x.call(this,n,c)},mouseenter(c){x.call(this,n,c)},mouseleave(c){x.call(this,n,c)}},children:(c,m)=>{var p=Se(),g=ee(p);xe(g,n,"default",{},null),r(c,p)},$$slots:{default:!0}}))}const Ue=[{id:"dnn",term:"Deep Neural Network (DNN)",description:"Analyzes and extracts important details from an image, such as shapes, colors, and patterns. Based on the analysis, the DNN then predicts how to classify the image.",models:[{name:"DINOv2",link:"https://github.com/facebookresearch/dinov2"}]},{id:"llm",term:"Large Language Model (LLM)",description:"A type of artificial intelligence trained on vast amounts of images and text to understand and generate human-like language while also interpreting visual information.  They can analyze and describe images, answer questions about visual content, and generate text based on both visual and textual inputs. This allows them to perform tasks like image captioning, visual question answering, and multimodal reasoning.",models:[{name:"MiniCPM-V-2_6",link:"https://huggingface.co/openbmb/MiniCPM-V-2_6"},{name:"Qwen2-VL-2B-Instruct",link:"https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct"}]},{id:"ml",term:"Machine Learning (ML)",description:"A type of artificial intelligence where computers learn patterns from data instead of being explicitly programmed."},{id:"ner",term:"Named Entity Recognition (NER)",description:"An NLP technique that identifies different types of entities such as locations, names, dates, and times by analyzing the surrounding context and linguistic patterns rather than relying solely on predefined rules like capitalization or keyword proximity.",models:[{name:"English NER in Flair",link:"https://huggingface.co/flair/ner-english-ontonotes-large"}]},{id:"nlp",term:"Natural Language Processing (NLP)",description:"A set of artificial intelligence techniques that enable computers to identify, understand, interpret, and generate human language."},{id:"ocr",term:"Optical Character Recognition (OCR)",description:"A technology that detects and extracts text from images or scanned documents, converting it into machine-readable text.",models:[{name:"Amazon Textract",link:"https://aws.amazon.com/textract/"}]},{id:"text",term:"Text Model",description:"An artificial intelligence system designed to process, generate, or analyze human language using machine learning techniques. In this case,  a sentence transformer that converts sentences into numerical embeddings, enabling tasks like semantic search, similarity comparison, and clustering.",models:[{name:"all-mpnet-base-v2",link:"https://huggingface.co/sentence-transformers/all-mpnet-base-v2"}]}],Ve={ms_sex_llm_v1:"An LLM extracts sex information found on individual pages. Those values are also aggregated to predict the sex of the A-File holder",ms_form_titles_llm_v1:"Identified by the LLM, this is a full list of every form number and corresponding title within the entire A-File",ms_countries_nlp_v1:"Collated list of all countries identifed on the page by the NLP model harnessing NER. These lists are also aggregated at the A-File level",ms_ocr_v1:"All key word searchable text extracted from the page through OCR",ms_form_title_llm_v1:"Form numbers and corresponding titles identified by the LLM",ms_years_nlp_v1:"Identified with a BiLSTM, a type of NLP model harnessing NER, this model compiles every year that appears on a page",ms_doctype_v1:"A DNN processes each scanned document and categorizes it into one of four types: Form, Letter, Photograph, or Misc",nationality_llm_v1:"Nationality data of A-File holder, as identified and extracted by the LLM from form G325",reason_llm_v1:"Reason supplied by A-File holder for submitting form G325, as identified and extracted by LLM from form G325",occupation_llm_v1:"Occupation data of A-File holder, as identified and extracted by the LLM from form G325",complexion_llm_v1:"Biometric data required by some Certificates of Naturalization as identified and extracted by LLM",marital_status_llm_v1:"Marital status listed on Certificates of Naturalization, as identified and extracted by LLM",residence_naturlization_llm_v1:"Place of residence for certificate holder at the time of naturalization, as identified and extracted from Certificate of Naturalization by LLM",year_naturalization_llm_v1:"Year of naturalization for certificate holder as listed on Certificates of Naturalization, as identified and extracted by LLM"};var Qe=k("<!>&nbsp;",1),Xe=k('<span class="block py-2 font-mono text-xs">M/S models used: <!></span>'),Je=k('<dl class="w-full"><dt class="text-base font-semibold"><!> </dt> <dd class="py-2 leading-normal"> <!></dd></dl>'),Ke=k('<div class="flex gap-6 py-4"><div class="min-w-56"><!></div> <div><p class="max-w-[85ch] text-sm"> </p></div></div>'),Ze=k(`<!> <div class="max-w-[85ch]"><h1 class="mb-8 md:text-5xl">M/S Data Guide</h1> <div class="my-6"><h2 class="mb-2">Contents</h2> <ul class="list-inside list-disc [&amp;_ul]:list-[revert]"><li class="pt-2"><a href="#introduction">Introduction</a> <ul class="list-inside indent-4"><li class="pt-2"><a href="#training-set">Training Set</a></li> <li class="pt-2"><a href="#metadata-sources">Metadata Sources</a></li> <li class="pt-2"><a href="#metadata-note">Metadata on Sex and Complexion</a></li></ul></li> <li class="pt-2"><a href="#methods-glossary">Methods Glossary</a></li> <li class="pt-2"><a href="#tags">Data Provenance Tags</a></li></ul></div> <h2 id="introduction" class="my-4">Introduction</h2> <h3 id="training-set" class="mb-4 mt-6 text-base uppercase tracking-widest md:text-lg">Training Set</h3> <p class="py-2">This M/S Reading Room prototype allows users to search the test set of A-Files using metadata
		derived from two sources: the publicly available U.S. National Archives (NARA) catalog metadata,
		and metadata based on information extracted by our machine learning models.</p> <p class="py-2">Most of the training set is made up of files ordered from the Kansas City regional office of
		NARA, where the vast majority of public domain A-Files are held. While these A-files are in the
		public domain (A-files enter the public domain when the birth year of the file holder is at
		least 100 years in the past), NARA reviews each file for third-party privacy violations and
		redacts information as necessary. NARA levies a processing fee of either $27 or $40 (depending
		on the file holder’s year of birth) for each A-File ordered online. Faced with a catalog of a
		million files and a budget for 550 files, we made selections designed to achieve regional and
		temporal breadth, with additional files for two countries of origin (Mexico and Haiti) to enable
		greater depth of research. The remainder of the training set comes from <!> and a relatively small number of A-Files obtained from USCIS via that agency’s Electronic Reading
		Room and our Freedom of Information Act (FOIA) requests for the A-Files of verifiably deceased individuals.
		The USCIS files are being used for model training purposes only and are not included in this prototype.</p> <h3 id="metadata-sources" class="mb-4 mt-6 text-base uppercase tracking-widest md:text-lg">Metadata Sources</h3> <p class="py-2">The <!> for A-Files varies. Every five years, files that have entered the public domain are transferred
		from the federal agency governing immigration (INS or USCIS) to NARA. At the time of the transfer,
		the agencies determine which metadata fields attached to the files will be transferred, resulting
		in variability in metadata across the entire NARA catalog (approximately 1.6 million A-Files). The
		basic metadata always available from the NARA catalog for each A-File includes date of birth, first
		name, last name, and country of birth. Additional metadata from the NARA catalog can include sex,
		date of entry, port of entry, date file opened, father’s name, mother’s name, date and location of
		naturalization, naturalization certificate number, and class of admission. Reference tables are available
		to identify the agency codes used for <!>, <!>, and <!>.</p> <p class="py-2">The metadata created by the <span class="italic">Migrants and the State</span> project has been
		derived using a variety of machine learning methods deployed on a training set of 751 files
		comprising 37,396 individual pages. The model training involved two main stages: categorizing
		documents and extracting information from them. In general, our approach has prioritized
		training models to address a larger number of data identification tasks rather than attempting
		fewer tasks and refining the models to achieve peak accuracy. We started by developing a model
		(using <!>) to create image
		embeddings for every page in the training set. Members of the team reviewed each page to
		identify machine-generated errors and used that information to improve the model’s performance
		in distinguishing among <!>, <!>, <!>, and other kinds of materials (which we labelled <!>).</p> <p class="py-2">While the model identifying document types relied on the visual information on each A-File page,
		the ability to extract text information required the use of <!> to create a machine-readable text version of the training set. This text allows us to perform general
		text extraction (identifying the presence of text in the body of a document, similar to a keyword
		search) as well as extracting information from a specific location within a document (for example,
		a form title or answer to a question).</p> <p class="py-2">We used existing <!> techniques
		to extract information about the years and country names included in each A-File. For document-specific
		metadata extraction, we chose the G-325 and Certificate of Naturalization forms as our test cases
		for training models (using a Large Language Model approach) that can identify a form type and extract
		specific information from it. The information the models can extract from these forms (employment
		history, residential addresses, and biographical details) is valuable in its own right, but we are
		also attempting to develop a scalable technique that can later be applied to other forms.</p> <h3 id="metadata-note" class="mb-4 mt-6 text-base uppercase tracking-widest md:text-lg">Metadata on Sex and Complexion</h3> <p class="py-2">The NARA catalog metadata, compiled as a limited subset of the metadata created by INS / USCIS,
		often lacks entries corresponding to the field designating biological sex. But information on
		biological sex, along with other biometric data required by government forms, appears frequently
		across documents in A-Files. We used an open-source <!> to extract this data from the individual pages of A-Files, typically from forms where a biological
		sex designation (e.g. Male / Female) is required. The model then aggregates the total number of identifications
		for biological sex and predicts the sex of the A-File holder. The LLM derived metadata is not 100%
		accurate, and the prototype allows you to compare NARA’s metadata for sex with the metadata extracted
		by the M/S trained LLM.</p> <p class="py-2">Extracting metadata to reflect historical government categories and bureaucratic norms around
		biological sex is relatively straightforward, given the binary categories of male and female
		then in use. Extracting metadata about ethno-racial categorization is more complex. This
		information often takes the form of indications of a file holder’s complexion, for which
		migrants and immigration officers used a variety of adjectives, many of them offensive to
		contemporary sensibilities. Currently, our models extract complexion data only from the
		Certificate of Naturalization forms. From the subset of data available in this prototype,
		complexion was recorded directly in these documents as <!>, <!>, or <!>, if at all. Our models attempt to record these terms exactly or add an <!> value if they do not find a complexion value on the page.</p> <p class="py-2">We crucially do not try to infer or profile a value through other cues in the A-File. Our aim is
		to organize the specific terms used by immigration bureaucracies over time as objects of study
		without intervention or curation. Nevertheless, please note that these metadata values, like
		others generated by machine learning in our corpus, are not 100% accurate. Additionally, more
		terms will likely emerge as we incorporate A-Files from broader time periods. For these reasons,
		we plan to solicit and incorporate user feedback to improve how we render this data with care.</p></div> <h2 id="methods-glossary" class="py-4">Methods Glossary</h2> <div class="my-10 grid w-full gap-4 sm:grid-cols-2 md:grid-cols-3"></div> <h2 id="tags" class="py-4">Data Provenance Tags</h2> <p class="max-w-[85ch] pb-6 pt-2">When exploring A-Files in the M/S Reading Room prototype, you'll see the following "data
	provenance tags" next to metadata fields by the A-File viewer. Clicking one will take you to the
	corresponding tag below, where you will find context for the source of that data. Definitions for
	terms like "LLM", "NLP", and "OCR" can be found above.</p> <div class="w-full"><div class="flex gap-6 py-4"><div class="min-w-56"><!></div> <div><p class="max-w-[85ch] text-sm">Fields with this tag come directly from the U.S. National Archives catalog.</p></div></div> <!></div>`,1);function mt($){var n=Ze(),u=ee(n);Be(u,{class:"mb-8",children:(t,s)=>{De(t,{href:_,children:(e,q)=>{i();var v=d("Home");r(e,v)},$$slots:{default:!0}})},$$slots:{default:!0}});var y=a(u,2),c=a(l(y),10),m=a(l(c));L(m,{size:"lg",href:"https://catalog.archives.gov/search?typeOfMaterials=Textual%20Records&availableOnline=true&recordGroupNumber=566",children:(t,s)=>{i();var e=d("A-Files made public by NARA’s San Francisco regional office");r(t,e)},$$slots:{default:!0}}),i(),o(c);var p=a(c,4),g=a(l(p));L(g,{size:"lg",href:"https://docs.google.com/document/u/0/d/1lnvN77_CaBnOzg9HBF-jtWTwJlisgKoD3E1RKHwQByQ/",children:(t,s)=>{i();var e=d("metadata available in the NARA catalog");r(t,e)},$$slots:{default:!0}});var h=a(g,2);L(h,{size:"lg",href:"https://docs.google.com/spreadsheets/d/1LfmR0QLmalz_6rJT_37QjRyFggsCXXOl5e24nOuw6XI/",children:(t,s)=>{i();var e=d("countries");r(t,e)},$$slots:{default:!0}});var F=a(h,2);L(F,{size:"lg",href:"https://docs.google.com/spreadsheets/d/1Zhs2RUElDE96EJmXGOCogfUdukj6mpbhWwsd6e2Sb34/",children:(t,s)=>{i();var e=d("ports of entry");r(t,e)},$$slots:{default:!0}});var D=a(F,2);L(D,{size:"lg",href:"https://docs.google.com/spreadsheets/d/1DH_0Wf0ALAB72D-v1Gu3sl7Iv_d6doSMv5-jj_r5Crc/",children:(t,s)=>{i();var e=d("naturalization locations");r(t,e)},$$slots:{default:!0}}),i(),o(p);var P=a(p,2),z=a(l(P),3);B(z,{size:"lg",href:"#dnn",children:(t,s)=>{i();var e=d("Deep Neural Networks");r(t,e)},$$slots:{default:!0}});var T=a(z,2);b(T,{size:"lg",href:`${_??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=form`,children:(t,s)=>{i();var e=d("forms");r(t,e)},$$slots:{default:!0}});var ie=a(T,2);b(ie,{size:"lg",href:`${_??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=letter`,children:(t,s)=>{i();var e=d("letters");r(t,e)},$$slots:{default:!0}});var re=a(ie,2);b(re,{size:"lg",href:`${_??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=photograph`,children:(t,s)=>{i();var e=d("photographs");r(t,e)},$$slots:{default:!0}});var Ae=a(re,2);b(Ae,{size:"lg",href:`${_??""}/results/page?query=&limit_fields.doctype.ms_doctype_v1=misc`,children:(t,s)=>{i();var e=d("“misc”");r(t,e)},$$slots:{default:!0}}),i(),o(P);var G=a(P,2),ze=a(l(G));B(ze,{size:"lg",href:"#ocr",children:(t,s)=>{i();var e=d("Optical Character Recognition (OCR)");r(t,e)},$$slots:{default:!0}}),i(),o(G);var j=a(G,2),Le=a(l(j));B(Le,{size:"lg",href:"#ner",children:(t,s)=>{i();var e=d("Named Entity Recognition (NER)");r(t,e)},$$slots:{default:!0}}),i(),o(j);var W=a(j,4),Me=a(l(W));B(Me,{size:"lg",href:"#llm",children:(t,s)=>{i();var e=d("Large Language Model (LLM)");r(t,e)},$$slots:{default:!0}}),i(),o(W);var se=a(W,2),oe=a(l(se));b(oe,{size:"lg",href:`${_??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=medium`,children:(t,s)=>{i();var e=d("medium");r(t,e)},$$slots:{default:!0}});var ne=a(oe,2);b(ne,{size:"lg",href:`${_??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=dark`,children:(t,s)=>{i();var e=d("dark");r(t,e)},$$slots:{default:!0}});var le=a(ne,2);b(le,{size:"lg",href:`${_??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=fair`,children:(t,s)=>{i();var e=d("fair");r(t,e)},$$slots:{default:!0}});var Ne=a(le,2);b(Ne,{size:"lg",href:`${_??""}/results/natcert?selectedFields=fields.certificate_naturalization.complexion.complexion_llm_v1&query=n.a`,children:(t,s)=>{i();var e=d("N.A");r(t,e)},$$slots:{default:!0}}),i(),o(se),i(2),o(y);var H=a(y,4);Z(H,5,()=>Ue,Y,(t,s)=>{je(t,{get id(){return f(s).id},children:(e,q)=>{var v=Je(),w=l(v),I=l(w);we(I,{size:20,class:"inline"});var V=a(I);o(w);var R=a(w,2),S=l(R),Q=a(S);be(Q,()=>f(s).models,X=>{var O=Xe(),J=a(l(O));Z(J,1,()=>f(s).models,Y,(me,C)=>{var he=Qe(),Pe=ee(he);L(Pe,{size:"sm",get href(){return f(C).link},children:(Re,Ye)=>{i();var fe=d();A(()=>M(fe,f(C).name)),r(Re,fe)},$$slots:{default:!0}}),i(),r(me,he)}),o(O),r(X,O)}),o(R),o(v),A(()=>{M(V,` ${f(s).term??""}`),M(S,`${f(s).description??""} `)}),r(e,v)},$$slots:{default:!0}})}),o(H);var de=a(H,6),U=l(de),ce=l(U),ke=l(ce);ae(ke,{href:"#nara",children:(t,s)=>{ye(t,{interactive:!0,id:"nara",type:"blue",children:(e,q)=>{i();var v=d("#nara");r(e,v)},$$slots:{default:!0}})},$$slots:{default:!0}}),o(ce),i(2),o(U);var Fe=a(U,2);Z(Fe,1,()=>Object.entries(Ve),Y,(t,s)=>{let e=()=>f(s)[0],q=()=>f(s)[1];var v=Ke(),w=l(v),I=l(w),V=Oe(()=>`#${e()}`);ae(I,{get href(){return f(V)},children:(X,O)=>{ye(X,{interactive:!0,get id(){return e()},type:"green",children:(J,me)=>{i();var C=d();A(()=>M(C,`#${e()??""}`)),r(J,C)},$$slots:{default:!0}})},$$slots:{default:!0}}),o(w);var R=a(w,2),S=l(R),Q=l(S,!0);o(S),o(R),o(v),A(()=>M(Q,q())),r(t,v)}),o(de),r($,n)}export{mt as component};
